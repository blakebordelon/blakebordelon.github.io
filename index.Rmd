---
title: ""
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


<div class = "row">
<div class = "col-md-5">
<br><br>
![A recent picture of me](IMG_1952.jpg){width=100%}
</div>
<div class = "col-md-6">
<br><br>
I am an applied math PhD student in the [Pehlevan Group](https://pehlevan.seas.harvard.edu/) at Harvard. My research interests lie in the convex hull of machine learning, statistical physics, and theoretical neuroscience. Before graduate school, I studied physics, engineering, and computer science at [Washington University in St. Louis](https://wustl.edu/). 

My works can be found on [Google Scholar](https://scholar.google.com/citations?user=yeQ8_pgAAAAJ&hl=en). I also occasionally post new preprints on [twitter](https://twitter.com/blake__bordelon).

<br><br>
</div>
<br><br>
</div>

### Recent News
+ Recent talk at Harvard CMSA on [scaling limits and scaling laws](https://www.youtube.com/watch?v=0998FJhPdj8&t=3205s).
+ New preprint on how [feature learning can alter power law exponents](https://arxiv.org/abs/2409.17858) of neural scaling laws.
+ Our work on infinite width and depth [limits in transformers](https://arxiv.org/abs/2405.15712) will be at Neurips 2024. 
+ I recenly helped teach at Analytical Connectionism Summer School 2024. Check out some [Google colab exercises](https://colab.research.google.com/drive/1Uqbg5anUtyWNKnZBoSce6SVn0Ehh8f_W?authuser=1) on mean field theory, random matrix theory, and scaling limits of neural networks. 
+ I gave a long 1:45 talk at ICTP Junior Theoretical Neuroscientist Workshops about several recent papers. [The recording can be found here](https://video.ictp.it/WEB/2024/2024_06_03-smr3943/2024_06_04-14_15-smr3943.mp4). 
+ Some blog posts ( [part 1](https://kempnerinstitute.harvard.edu/research/deeper-learning/infinite-limits-of-neural-networks/) and [part 2](https://kempnerinstitute.harvard.edu/research/deeper-learning/a-dynamical-model-of-neural-scaling-laws/)) about our recent works on infinite limits and neural scaling laws were posted by the Kempner instute.


### Reviewed Publications


- **Bordelon**, Chaudhry, Pehlevan ["Infinite Limits of Multi-head Transformer Dynamics"](https://arxiv.org/abs/2405.15712). *Neurips* 2024. 


- **Bordelon**, Atanasov, Pehlevan ["A Dynamical Model of Neural Scaling Laws"](https://arxiv.org/abs/2402.01092) *ICML* 2024. 

- **Bordelon**, Noci, Li, Hanin, Pehlevan ["Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit"](https://openreview.net/forum?id=KZJehvRKGD) *ICLR* 2024

- Kumar, **Bordelon**, Gershman, Pehlevan ["Grokking as the Transition from Lazy to Rich Training Dynamics"](https://openreview.net/forum?id=vt5mnLVIVo) *ICLR* 2024

- **Bordelon**, Pehlevan ["Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks"](https://openreview.net/forum?id=fKwG6grp8o), *Neurips* spotlight 2023.
 
- **Bordelon**, Masset, Kuo, Pehlevan ["Loss Dynamics of Temporal Difference Reinforcement Learning"](https://openreview.net/forum?id=Tj0eXVPnRX), *Neurips* 2023.

- Vyas, Atanasov, **Bordelon**, Morwani, Sainathan, Pehlevan [Feature-Learning Networks are consistent across widths at realistic scales](https://openreview.net/forum?id=LTdfYIvbHc) *Neurips* 2023 (equal contribution of first three authors).


- **Bordelon**, Pehlevan ["The Influence of Learning Rule on Representation Dynamics in Wide Neural Networks"](https://openreview.net/forum?id=nZ2NtpolC5-), *ICLR* 2023. Notable-top 25%.

- Atanasov, **Bordelon**, Sainathan, Pehlevan [The Onset of Variance-Limited Behavior for Networks in the Lazy and Rich Regimes](https://openreview.net/forum?id=JLINxPOVTh7) (Equal Contribution), *ICLR* 2023. 

- **Bordelon**, Pehlevan ["Population Codes Enable Learning from Few Examples By Shaping Inductive Bias"](https://elifesciences.org/articles/78606), *Elife* 2022.

- **Bordelon**, Pehlevan [Self-consistent Dynamical Field Theory of Kernel Evolution in Wide Neural Networks](https://openreview.net/forum?id=sipwrPCrIS). *Neurips* 2022. 

- Atanasov, **Bordelon**, Pehlevan ["Neural Networks as Kernel Learners: The Silent Alignment Effect"](https://arxiv.org/abs/2111.00034) (Equal Contribution), *ICLR* 2022.

- Farrell, **Bordelon**, Trivedi, Pehlevan ["Capacity of Group-invariant Linear Readouts from Equivariant Representations: How Many Objects can be Linearly Classified Under All Possible Views?"](https://arxiv.org/abs/2110.07472) (Equal Contribution), *ICLR* 2022.

- **Bordelon**, Pehlevan ["Learning Curves for SGD on Structured Features
"](https://openreview.net/forum?id=WPI2vbkAl3Q), *ICLR* 2022.

- Canatar, **Bordelon**, Pehlevan ["Out-of-Distribution Generalization in Kernel Regression"](https://arxiv.org/abs/2106.02261), *Neurips* 2021.

- Canatar, **Bordelon**, and Pehlevan, [“Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks”](https://www.nature.com/articles/s41467-021-23103-1) *Nature Communications* 2021. 

- **Bordelon**, Canatar, and Pehlevan, [“Spectrum Dependent Learning Curves in Kernel Regression and Wide Neural Networks,”](https://arxiv.org/abs/2002.02561) *ICML*, 2020.

- Atkinson, Mahzoon, Keim, **Bordelon**, Pruitt, Charity, and Dickhoff [“Dispersive optical model analysis of Pb-208 generating a neutron-skin prediction beyond the mean field,”](https://journals.aps.org/prc/abstract/10.1103/PhysRevC.101.044303)
*Phys. Rev. C* 101, 044303, 2020

- Bagley, **Bordelon**, Moseley, Wessel ["Pre-Synaptic Pool Modification (PSPM): A supervised learning procedure for recurrent spiking neural networks""](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0229083) *PLOS ONE*, 2020. 


### Preprints


- **Bordelon**, Atanasov, Pehlevan ["How Feature Learning Can Improve Neural Scaling Laws"](https://arxiv.org/abs/2409.17858) 2024. 

- Shan, **Bordelon** ["Rapid Feature Evolution Accelerates Learning in Neural Networks
"](https://arxiv.org/abs/2105.14301) (Equal Contribution) 2021.





