---
title: ""
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


<div class = "row">
<div class = "col-md-5">
<br><br>
![A recent picture of me](IMG_1952.jpg){width=100%}
</div>
<div class = "col-md-6">
<br><br>

### Starting at UT Austin 2026, Hiring Postdocs and PhD students
I will be moving to UT Austin [Oden Institute](https://oden.utexas.edu/) and [Department of Neuroscience](https://neuroscience.utexas.edu/). I am hiring postocs [(link to apply)](https://oden.utexas.edu/programs-and-awards/odonnell-jr-postdoc-fellowship/) and also [PhD students](https://oden.utexas.edu/academics/prospective-students/how-to-apply/). If you are interested in working with me, indicate your interest in the application. Feel free to contact me via [email](mailto:blake_bordelon@utexas.edu)

#### Research Topics 

Machine learning, theoretical/computational neuroscience, high dimensional statistics and optimization.

<br><br>
</div>
<br><br>
</div>

### Background
I am a postdoctoral research fellow at Harvard [Center for Mathematical Sciences and Applications (CMSA)](https://cmsa.fas.harvard.edu/). In May 2025, I defended my PhD in Applied Math at Harvard where I worked with [Pehlevan Group](https://pehlevan.seas.harvard.edu/). My research interests lie in the convex hull of machine learning, statistical physics, and theoretical neuroscience. Before graduate school, I studied physics, engineering, and computer science at [Washington University in St. Louis](https://wustl.edu/). 

My works can be found on [Google Scholar](https://scholar.google.com/citations?user=yeQ8_pgAAAAJ&hl=en). I also occasionally post new preprints on [twitter](https://twitter.com/blake__bordelon).


### Recent News
+ New preprint on [theory of depth and width scaling laws for in-context learning in linear transformers](https://arxiv.org/abs/2510.01098) with Mary Letey and Cengiz Pehlevan. 
+ Collaboration with Cerebras on Depth scaling in Transformers [accepted as Neurips paper](https://arxiv.org/abs/2505.01618). 
+ Started as a postdoc at Harvard CMSA in August 2025. 
+ I defended my PhD in May 2025. My thesis [Learning in Large Neural Networks](https://www.proquest.com/pqdtglobal/docview/3216897077/76DD51AFE0B948D1PQ/1?accountid=11311&sourcetype=Dissertations%20&%20Theses) can be found on Proquest. 
+ [Talk](https://www.youtube.com/watch?v=WcWFFiPRslM&t=502s) at IPAM Workshop on Deep Learning Theory and Practice 
+ Recent talk at Harvard CMSA on [scaling limits and scaling laws](https://www.youtube.com/watch?v=0998FJhPdj8&t=3205s).
+ New preprint on how [feature learning can alter power law exponents](https://arxiv.org/abs/2409.17858) of neural scaling laws.
+ Our work on infinite width and depth [limits in transformers](https://arxiv.org/abs/2405.15712) will be at Neurips 2024. 


### Reviewed Publications


- **Bordelon**, Chaudhry, Pehlevan ["Infinite Limits of Multi-head Transformer Dynamics"](https://arxiv.org/abs/2405.15712). *Neurips* 2024. 


- **Bordelon**, Atanasov, Pehlevan ["A Dynamical Model of Neural Scaling Laws"](https://arxiv.org/abs/2402.01092) *ICML* 2024. 

- **Bordelon**, Noci, Li, Hanin, Pehlevan ["Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit"](https://openreview.net/forum?id=KZJehvRKGD) *ICLR* 2024

- Kumar, **Bordelon**, Gershman, Pehlevan ["Grokking as the Transition from Lazy to Rich Training Dynamics"](https://openreview.net/forum?id=vt5mnLVIVo) *ICLR* 2024

- **Bordelon**, Pehlevan ["Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks"](https://openreview.net/forum?id=fKwG6grp8o), *Neurips* spotlight 2023.
 
- **Bordelon**, Masset, Kuo, Pehlevan ["Loss Dynamics of Temporal Difference Reinforcement Learning"](https://openreview.net/forum?id=Tj0eXVPnRX), *Neurips* 2023.

- Vyas, Atanasov, **Bordelon**, Morwani, Sainathan, Pehlevan [Feature-Learning Networks are consistent across widths at realistic scales](https://openreview.net/forum?id=LTdfYIvbHc) *Neurips* 2023 (equal contribution of first three authors).


- **Bordelon**, Pehlevan ["The Influence of Learning Rule on Representation Dynamics in Wide Neural Networks"](https://openreview.net/forum?id=nZ2NtpolC5-), *ICLR* 2023. Notable-top 25%.

- Atanasov, **Bordelon**, Sainathan, Pehlevan [The Onset of Variance-Limited Behavior for Networks in the Lazy and Rich Regimes](https://openreview.net/forum?id=JLINxPOVTh7) (Equal Contribution), *ICLR* 2023. 

- **Bordelon**, Pehlevan ["Population Codes Enable Learning from Few Examples By Shaping Inductive Bias"](https://elifesciences.org/articles/78606), *Elife* 2022.

- **Bordelon**, Pehlevan [Self-consistent Dynamical Field Theory of Kernel Evolution in Wide Neural Networks](https://openreview.net/forum?id=sipwrPCrIS). *Neurips* 2022. 

- Atanasov, **Bordelon**, Pehlevan ["Neural Networks as Kernel Learners: The Silent Alignment Effect"](https://arxiv.org/abs/2111.00034) (Equal Contribution), *ICLR* 2022.

- Farrell, **Bordelon**, Trivedi, Pehlevan ["Capacity of Group-invariant Linear Readouts from Equivariant Representations: How Many Objects can be Linearly Classified Under All Possible Views?"](https://arxiv.org/abs/2110.07472) (Equal Contribution), *ICLR* 2022.

- **Bordelon**, Pehlevan ["Learning Curves for SGD on Structured Features
"](https://openreview.net/forum?id=WPI2vbkAl3Q), *ICLR* 2022.

- Canatar, **Bordelon**, Pehlevan ["Out-of-Distribution Generalization in Kernel Regression"](https://arxiv.org/abs/2106.02261), *Neurips* 2021.

- Canatar, **Bordelon**, and Pehlevan, [“Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks”](https://www.nature.com/articles/s41467-021-23103-1) *Nature Communications* 2021. 

- **Bordelon**, Canatar, and Pehlevan, [“Spectrum Dependent Learning Curves in Kernel Regression and Wide Neural Networks,”](https://arxiv.org/abs/2002.02561) *ICML*, 2020.

- Atkinson, Mahzoon, Keim, **Bordelon**, Pruitt, Charity, and Dickhoff [“Dispersive optical model analysis of Pb-208 generating a neutron-skin prediction beyond the mean field,”](https://journals.aps.org/prc/abstract/10.1103/PhysRevC.101.044303)
*Phys. Rev. C* 101, 044303, 2020

- Bagley, **Bordelon**, Moseley, Wessel ["Pre-Synaptic Pool Modification (PSPM): A supervised learning procedure for recurrent spiking neural networks""](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0229083) *PLOS ONE*, 2020. 


### Preprints


- **Bordelon**, Atanasov, Pehlevan ["How Feature Learning Can Improve Neural Scaling Laws"](https://arxiv.org/abs/2409.17858) 2024. 


- Kumar, Anker, Spector, **Bordelon**, Muenninghof, Paul, Pehlevan, Re, Raghunathan ["Scaling Laws for Precision"](https://arxiv.org/abs/2411.04330) 2024.

- Kumar, **Bordelon**, Pehlevan, Murthy, Gershman ["Do Mice Grok? Glimpses of Hidden Progress During Overtraining in Sensory Cortex"](https://arxiv.org/abs/2411.03541) 2024.

- Shan, **Bordelon** ["Rapid Feature Evolution Accelerates Learning in Neural Networks
"](https://arxiv.org/abs/2105.14301) (Equal Contribution) 2021.





